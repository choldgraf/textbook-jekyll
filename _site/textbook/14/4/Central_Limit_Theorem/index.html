<!doctype html>
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>The Central Limit Theorem ### - Data 8 Textbook</title>
<meta name="description" content="">



<meta property="og:type" content="website">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Data 8 Textbook">
<meta property="og:title" content="The Central Limit Theorem ###">
<meta property="og:url" content="http://localhost:4000/jupyterhub-for-education-template/textbook/14/4/Central_Limit_Theorem/">












  

  


<link rel="canonical" href="http://localhost:4000/jupyterhub-for-education-template/textbook/14/4/Central_Limit_Theorem/">







  <script type="application/ld+json">
    {
      "@context": "http://schema.org",
      "@type": "Person",
      "name": "Berkeley DSEP",
      "url": "http://localhost:4000/jupyterhub-for-education-template",
      "sameAs": null
    }
  </script>







<!-- end _includes/seo.html -->


<link href="/jupyterhub-for-education-template/feed.xml" type="application/atom+xml" rel="alternate" title="Data 8 Textbook Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/jupyterhub-for-education-template/assets/css/main.css">

<!--[if lte IE 9]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->


    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

    <link rel="stylesheet" href="/jupyterhub-for-education-template/assets/css/notebook-markdown.css">
    <link rel="stylesheet" href="/jupyterhub-for-education-template/assets/css/custom.css">
    <link rel="shortcut icon" type="image/png" href="/jupyterhub-for-education-template/favicon.png">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [['$','$']],
        processEscapes: false
      }
    });
    // MathJax Configuration
    </script>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript">
    </script>
    <script type="text/javascript">
    function getQueryVariable(variable)
    {
           var query = window.location.search.substring(1);
           var vars = query.split("&");
           for (var i=0;i<vars.length;i++) {
                   var pair = vars[i].split("=");
                   if(pair[0] == variable){return pair[1];}
           }
           return(false);
    }
    window.onload = function() {
        window.hubUrl = getQueryVariable("hubUrl");
        if (window.hubUrl !== false) {
            if (window.hubUrl.indexOf('http') < 0) {
                window.hubUrl = 'http://' + window.hubUrl;
            }
            link = $("a.interact-button")[0];
            if (link !== undefined) {
                var href = link.getAttribute('href');
                href = href.replace("http://datahub.berkeley.edu", window.hubUrl);
                link.setAttribute('href', href);
            }
        }
    }
    </script>
  </head>

  <body class="layout--textbook">

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    
    <div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        <a class="site-title" href="/jupyterhub-for-education-template/">Data 8 Textbook</a>
        <ul class="visible-links">
          
            
            <li class="masthead__menu-item">
              <a href="http://localhost:4000/jupyterhub-for-education-template/about/" >About</a>
            </li>
          
            
            <li class="masthead__menu-item">
              <a href="http://localhost:4000/jupyterhub-for-education-template/syllabus/" >Syllabus</a>
            </li>
          
            
            <li class="masthead__menu-item">
              <a href="http://localhost:4000/jupyterhub-for-education-template/references/" >Resources</a>
            </li>
          
            
            <li class="masthead__menu-item">
              <a href="http://localhost:4000/jupyterhub-for-education-template/textbook/01/what-is-data-science" >Textbook</a>
            </li>
          
          <li class="masthead__menu-item">
            <a href="http://datahub.berkeley.edu">DataHub</a>
          </li>
        </ul>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle Menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>

    

    <div class="initial-content">
      



<div id="main" role="main">
  
  <div class="sidebar sticky">
  
  
    
      
      
      
    
    
      

<nav class="nav__list">
  
  <input id="ac-toc" name="accordion-toc" type="checkbox" />
  <label for="ac-toc">Toggle Menu</label>
  <ul class="nav__items">
    
      <li>
        
          
          

          <a href="/jupyterhub-for-education-template/textbook/01/what-is-data-science"><span class="nav__sub-title">1. Data Science</span></a>
        

        
        <ul>
          
            
            

            
            

            

            <li><a href="/jupyterhub-for-education-template/textbook/01/1/intro" class="level_1">1.1 Introduction</a></li>
          
            
            

            
            

            

            <li><a href="/jupyterhub-for-education-template/textbook/01/1/1/computational-tools" class="level_2">1.1.1 Computational Tools</a></li>
          
            
            

            
            

            

            <li><a href="/jupyterhub-for-education-template/textbook/01/1/2/statistical-techniques" class="level_2">1.1.2 Statistical Techniques</a></li>
          
            
            

            
            

            

            <li><a href="/jupyterhub-for-education-template/textbook/01/2/why-data-science" class="level_1">1.2 Why Data Science?</a></li>
          
            
            

            
            

            

            <li><a href="/jupyterhub-for-education-template/textbook/01/3/Plotting_the_Classics" class="level_1">1.3 Plotting the Classics</a></li>
          
            
            

            
            

            

            <li><a href="/jupyterhub-for-education-template/textbook/01/3/1/Literary_Characters" class="level_2">1.3.1 Literary Characters</a></li>
          
            
            

            
            

            

            <li><a href="/jupyterhub-for-education-template/textbook/01/3/2/Another_Kind_Of_Character" class="level_2">1.3.2 Another Kind of Character</a></li>
          
        </ul>
        
      </li>
    
      <li>
        
          
          

          <a href="/jupyterhub-for-education-template/textbook/02/causality-and-experiments"><span class="nav__sub-title">2. Causality and Experiments</span></a>
        

        
        <ul>
          
            
            

            
            

            

            <li><a href="/jupyterhub-for-education-template/textbook/02/1/observation-and-visualization-john-snow-and-the-broad-street-pump" class="level_1">2.1 John Snow and the Broad Street Pump</a></li>
          
            
            

            
            

            

            <li><a href="/jupyterhub-for-education-template/textbook/02/2/snow-s-grand-experiment" class="level_1">2.2 Snow’s “Grand Experiment”</a></li>
          
            
            

            
            

            

            <li><a href="/jupyterhub-for-education-template/textbook/02/3/establishing-causality" class="level_1">2.3 Establishing Causality</a></li>
          
            
            

            
            

            

            <li><a href="/jupyterhub-for-education-template/textbook/02/4/randomization" class="level_1">2.4 Randomization</a></li>
          
            
            

            
            

            

            <li><a href="/jupyterhub-for-education-template/textbook/02/5/endnote" class="level_1">2.5 Endnote</a></li>
          
        </ul>
        
      </li>
    
      <li>
        
          
          

          <a href="/jupyterhub-for-education-template/textbook/03/programming-in-python"><span class="nav__sub-title">3. Programming in Python</span></a>
        

        
        <ul>
          
            
            

            
            

            

            <li><a href="/jupyterhub-for-education-template/textbook/03/1/Expressions" class="level_1">3.1 Expressions</a></li>
          
            
            

            
            

            

            <li><a href="/jupyterhub-for-education-template/textbook/03/2/Names" class="level_1">3.2 Names</a></li>
          
            
            

            
            

            

            <li><a href="/jupyterhub-for-education-template/textbook/03/2/1/Growth" class="level_2">3.2.1 Example: Growth Rates</a></li>
          
            
            

            
            

            

            <li><a href="/jupyterhub-for-education-template/textbook/03/3/Calls" class="level_1">3.3 Call Expressions</a></li>
          
            
            

            
            

            

            <li><a href="/jupyterhub-for-education-template/textbook/03/4/Introduction_to_Tables" class="level_1">3.4 Introduction to Tables</a></li>
          
        </ul>
        
      </li>
    
      <li>
        
          
          

          <a href="/jupyterhub-for-education-template/textbook/04/Types"><span class="nav__sub-title">4. Data Types</span></a>
        

        
        <ul>
          
            
            

            
            

            

            <li><a href="/jupyterhub-for-education-template/textbook/04/1/Numbers" class="level_1">4.1 Numbers</a></li>
          
            
            

            
            

            

            <li><a href="/jupyterhub-for-education-template/textbook/04/2/Strings" class="level_1">4.2 Strings</a></li>
          
            
            

            
            

            

            <li><a href="/jupyterhub-for-education-template/textbook/04/2/1/String_Methods" class="level_2">4.2.1 String Methods</a></li>
          
            
            

            
            

            

            <li><a href="/jupyterhub-for-education-template/textbook/04/3/Comparison" class="level_1">4.3 Comparisons</a></li>
          
        </ul>
        
      </li>
    
      <li>
        
          
          

          <a href="/jupyterhub-for-education-template/textbook/05/Collections"><span class="nav__sub-title">5. Sequences</span></a>
        

        
        <ul>
          
            
            

            
            

            

            <li><a href="/jupyterhub-for-education-template/textbook/05/1/Arrays" class="level_1">5.1 Arrays</a></li>
          
            
            

            
            

            

            <li><a href="/jupyterhub-for-education-template/textbook/05/2/Ranges" class="level_1">5.2 Ranges</a></li>
          
            
            

            
            

            

            <li><a href="/jupyterhub-for-education-template/textbook/05/3/More_on_Arrays" class="level_1">5.3 More on Arrays</a></li>
          
        </ul>
        
      </li>
    
      <li>
        
          
          

          <a href="/jupyterhub-for-education-template/textbook/06/Tables"><span class="nav__sub-title">6. Tables</span></a>
        

        
        <ul>
          
            
            

            
            

            

            <li><a href="/jupyterhub-for-education-template/textbook/06/1/Sorting_Rows" class="level_1">6.1 Sorting Rows</a></li>
          
            
            

            
            

            

            <li><a href="/jupyterhub-for-education-template/textbook/06/2/Selecting_Rows" class="level_1">6.2 Selecting Rows</a></li>
          
            
            

            
            

            

            <li><a href="/jupyterhub-for-education-template/textbook/06/3/Example_Trends_in_the_Population_of_the_United_States" class="level_1">6.3 Example: Population Trends</a></li>
          
            
            

            
            

            

            <li><a href="/jupyterhub-for-education-template/textbook/06/4/Example_Gender_Ratio_in_the_US_Population" class="level_1">6.4 Example: Trends in Gender</a></li>
          
        </ul>
        
      </li>
    
      <li>
        
          
          

          <a href="/jupyterhub-for-education-template/textbook/07/Visualization"><span class="nav__sub-title">7. Visualization</span></a>
        

        
        <ul>
          
            
            

            
            

            

            <li><a href="/jupyterhub-for-education-template/textbook/07/1/Visualizing_Categorical_Distributions" class="level_1">7.1 Categorical Distributions</a></li>
          
            
            

            
            

            

            <li><a href="/jupyterhub-for-education-template/textbook/07/2/Visualizing_Numerical_Distributions" class="level_1">7.2 Numerical Distributions</a></li>
          
            
            

            
            

            

            <li><a href="/jupyterhub-for-education-template/textbook/07/3/Overlaid_Graphs" class="level_1">7.3 Overlaid Graphs</a></li>
          
        </ul>
        
      </li>
    
      <li>
        
          
          

          <a href="/jupyterhub-for-education-template/textbook/08/Functions_and_Tables"><span class="nav__sub-title">8. Functions and Tables</span></a>
        

        
        <ul>
          
            
            

            
            

            

            <li><a href="/jupyterhub-for-education-template/textbook/08/1/Applying_a_Function_to_a_Column" class="level_1">8.1 Applying Functions to Columns</a></li>
          
            
            

            
            

            

            <li><a href="/jupyterhub-for-education-template/textbook/08/2/Classifying_by_One_Variable" class="level_1">8.2 Classifying by One Variable</a></li>
          
            
            

            
            

            

            <li><a href="/jupyterhub-for-education-template/textbook/08/3/Cross-Classifying_by_More_than_One_Variable" class="level_1">8.3 Cross-Classifying</a></li>
          
            
            

            
            

            

            <li><a href="/jupyterhub-for-education-template/textbook/08/4/Joining_Tables_by_Columns" class="level_1">8.4 Joining Tables by Columns</a></li>
          
            
            

            
            

            

            <li><a href="/jupyterhub-for-education-template/textbook/08/5/Bike_Sharing_in_the_Bay_Area" class="level_1">8.5 Bike Sharing in the Bay Area</a></li>
          
        </ul>
        
      </li>
    
      <li>
        
          
          

          <a href="/jupyterhub-for-education-template/textbook/09/Randomness"><span class="nav__sub-title">9. Randomness</span></a>
        

        
        <ul>
          
            
            

            
            

            

            <li><a href="/jupyterhub-for-education-template/textbook/09/1/Conditional_Statements" class="level_1">9.1 Conditional Statements</a></li>
          
            
            

            
            

            

            <li><a href="/jupyterhub-for-education-template/textbook/09/2/Iteration" class="level_1">9.2 Iteration</a></li>
          
            
            

            
            

            

            <li><a href="/jupyterhub-for-education-template/textbook/09/3/Simulation" class="level_1">9.3 Simulation</a></li>
          
            
            

            
            

            

            <li><a href="/jupyterhub-for-education-template/textbook/09/4/Monty_Hall_Problem" class="level_1">9.4 The Monty Hall Problem</a></li>
          
            
            

            
            

            

            <li><a href="/jupyterhub-for-education-template/textbook/09/5/Finding_Probabilities" class="level_1">9.5 Finding Probabilities</a></li>
          
        </ul>
        
      </li>
    
      <li>
        
          
          

          <a href="/jupyterhub-for-education-template/textbook/10/Sampling_and_Empirical_Distributions"><span class="nav__sub-title">10. Sampling and Empirical Distributions</span></a>
        

        
        <ul>
          
            
            

            
            

            

            <li><a href="/jupyterhub-for-education-template/textbook/10/1/Empirical_Distributions" class="level_1">10.1 Empirical Distributions</a></li>
          
            
            

            
            

            

            <li><a href="/jupyterhub-for-education-template/textbook/10/2/Sampling_from_a_Population" class="level_1">10.2 Sampling from a Population</a></li>
          
            
            

            
            

            

            <li><a href="/jupyterhub-for-education-template/textbook/10/3/Empirical_Distribution_of_a_Statistic" class="level_1">10.3 Empirical Distibution of a Statistic</a></li>
          
        </ul>
        
      </li>
    
      <li>
        
          
          

          <a href="/jupyterhub-for-education-template/textbook/11/Testing_Hypotheses"><span class="nav__sub-title">11. Testing Hypotheses</span></a>
        

        
        <ul>
          
            
            

            
            

            

            <li><a href="/jupyterhub-for-education-template/textbook/11/1/Assessing_Models" class="level_1">11.1 Assessing Models</a></li>
          
            
            

            
            

            

            <li><a href="/jupyterhub-for-education-template/textbook/11/2/Multiple_Categories" class="level_1">11.2 Multiple Categories</a></li>
          
            
            

            
            

            

            <li><a href="/jupyterhub-for-education-template/textbook/11/3/Decisions_and_Uncertainty" class="level_1">11.3 Decisions and Uncertainty</a></li>
          
        </ul>
        
      </li>
    
      <li>
        
          
          

          <a href="/jupyterhub-for-education-template/textbook/12/Comparing_Two_Samples"><span class="nav__sub-title">12. Comparing Two Samples</span></a>
        

        
        <ul>
          
            
            

            
            

            

            <li><a href="/jupyterhub-for-education-template/textbook/12/1/AB_Testing" class="level_1">12.1 A/B Testing</a></li>
          
            
            

            
            

            

            <li><a href="/jupyterhub-for-education-template/textbook/12/2/Deflategate" class="level_1">12.2 Deflategate</a></li>
          
            
            

            
            

            

            <li><a href="/jupyterhub-for-education-template/textbook/12/3/Causality" class="level_1">12.3 Causality</a></li>
          
        </ul>
        
      </li>
    
      <li>
        
          
          

          <a href="/jupyterhub-for-education-template/textbook/13/Estimation"><span class="nav__sub-title">13. Estimation</span></a>
        

        
        <ul>
          
            
            

            
            

            

            <li><a href="/jupyterhub-for-education-template/textbook/13/1/Percentiles" class="level_1">13.1 Percentiles</a></li>
          
            
            

            
            

            

            <li><a href="/jupyterhub-for-education-template/textbook/13/2/Bootstrap" class="level_1">13.2 The Bootstrap</a></li>
          
            
            

            
            

            

            <li><a href="/jupyterhub-for-education-template/textbook/13/3/Confidence_Intervals" class="level_1">13.3 Confidence Intervals</a></li>
          
            
            

            
            

            

            <li><a href="/jupyterhub-for-education-template/textbook/13/4/Using_Confidence_Intervals" class="level_1">13.4 Using Confidence Intervals</a></li>
          
        </ul>
        
      </li>
    
      <li>
        
          
          

          <a href="/jupyterhub-for-education-template/textbook/14/Why_the_Mean_Matters"><span class="nav__sub-title">14. Why the Mean Matters</span></a>
        

        
        <ul>
          
            
            

            
            

            

            <li><a href="/jupyterhub-for-education-template/textbook/14/1/Properties_of_the_Mean" class="level_1">14.1 Properties of the Mean</a></li>
          
            
            

            
            

            

            <li><a href="/jupyterhub-for-education-template/textbook/14/2/Variability" class="level_1">14.2 Variability</a></li>
          
            
            

            
            

            

            <li><a href="/jupyterhub-for-education-template/textbook/14/3/SD_and_the_Normal_Curve" class="level_1">14.3 The SD and the Normal Curve</a></li>
          
            
            

            
            

            

            <li><a href="/jupyterhub-for-education-template/textbook/14/4/Central_Limit_Theorem" class="level_1">14.4 The Central Limit Theorem</a></li>
          
            
            

            
            

            

            <li><a href="/jupyterhub-for-education-template/textbook/14/5/Variability_of_the_Sample_Mean" class="level_1">14.5 The Variability of the Sample Mean</a></li>
          
            
            

            
            

            

            <li><a href="/jupyterhub-for-education-template/textbook/14/6/Choosing_a_Sample_Size" class="level_1">14.6 Choosing a Sample Size</a></li>
          
        </ul>
        
      </li>
    
      <li>
        
          
          

          <a href="/jupyterhub-for-education-template/textbook/15/Prediction"><span class="nav__sub-title">15. Prediction</span></a>
        

        
        <ul>
          
            
            

            
            

            

            <li><a href="/jupyterhub-for-education-template/textbook/15/1/Correlation" class="level_1">15.1 Correlation</a></li>
          
            
            

            
            

            

            <li><a href="/jupyterhub-for-education-template/textbook/15/2/Regression_Line" class="level_1">15.2 The Regression Line</a></li>
          
            
            

            
            

            

            <li><a href="/jupyterhub-for-education-template/textbook/15/3/Method_of_Least_Squares" class="level_1">15.3 The Method of Least Squares</a></li>
          
            
            

            
            

            

            <li><a href="/jupyterhub-for-education-template/textbook/15/4/Least_Squares_Regression" class="level_1">15.4 Least Squares Regression</a></li>
          
            
            

            
            

            

            <li><a href="/jupyterhub-for-education-template/textbook/15/5/Visual_Diagnostics" class="level_1">15.5 Visual Diagnostics</a></li>
          
            
            

            
            

            

            <li><a href="/jupyterhub-for-education-template/textbook/15/6/Numerical_Diagnostics" class="level_1">15.6 Numerical Diagnostics</a></li>
          
        </ul>
        
      </li>
    
      <li>
        
          
          

          <a href="/jupyterhub-for-education-template/textbook/16/Inference_for_Regression"><span class="nav__sub-title">16. Inference for Regression</span></a>
        

        
        <ul>
          
            
            

            
            

            

            <li><a href="/jupyterhub-for-education-template/textbook/16/1/Regression_Model" class="level_1">16.1 A Regression Model</a></li>
          
            
            

            
            

            

            <li><a href="/jupyterhub-for-education-template/textbook/16/2/Inference_for_the_True_Slope" class="level_1">16.2 Inference for the True Slope</a></li>
          
            
            

            
            

            

            <li><a href="/jupyterhub-for-education-template/textbook/16/3/Prediction_Intervals" class="level_1">16.3 Prediction Intervals</a></li>
          
        </ul>
        
      </li>
    
      <li>
        
          
          

          <a href="/jupyterhub-for-education-template/textbook/17/Classification"><span class="nav__sub-title">17. Classification</span></a>
        

        
        <ul>
          
            
            

            
            

            

            <li><a href="/jupyterhub-for-education-template/textbook/17/1/Nearest_Neighbors" class="level_1">17.1 Nearest Neighbors</a></li>
          
            
            

            
            

            

            <li><a href="/jupyterhub-for-education-template/textbook/17/2/Training_and_Testing" class="level_1">17.2 Training and Testing</a></li>
          
            
            

            
            

            

            <li><a href="/jupyterhub-for-education-template/textbook/17/3/Rows_of_Tables" class="level_1">17.3 Rows of Tables</a></li>
          
            
            

            
            

            

            <li><a href="/jupyterhub-for-education-template/textbook/17/4/Implementing_the_Classifier" class="level_1">17.4 Implementing the Classifier</a></li>
          
            
            

            
            

            

            <li><a href="/jupyterhub-for-education-template/textbook/17/5/Accuracy_of_the_Classifier" class="level_1">17.5 The Accuracy of the Classifier</a></li>
          
            
            

            
            

            

            <li><a href="/jupyterhub-for-education-template/textbook/17/6/Multiple_Regression" class="level_1">17.6 Multiple Regression</a></li>
          
        </ul>
        
      </li>
    
      <li>
        
          
          

          <a href="/jupyterhub-for-education-template/textbook/18/Updating_Predictions"><span class="nav__sub-title">18. Updating Predictions</span></a>
        

        
        <ul>
          
            
            

            
            

            

            <li><a href="/jupyterhub-for-education-template/textbook/18/1/More_Likely_than_Not_Binary_Classifier" class="level_1">18.1 A "More Likely Than Not" Binary Classifier</a></li>
          
            
            

            
            

            

            <li><a href="/jupyterhub-for-education-template/textbook/18/2/Making_Decisions" class="level_1">18.2 Making Decisions</a></li>
          
        </ul>
        
      </li>
    
  </ul>
</nav>

    
  
  </div>

  <article class="page" itemscope itemtype="http://schema.org/CreativeWork">
    <meta itemprop="headline" content="The Central Limit Theorem ###">
    
    
    
    <!-- INTERACT LINKS -->
    <a class="interact-button" href="http://datahub.berkeley.edu/hub/user-redirect/git-pull?repo=https://github.com/choldgraf/data8-textbook&amp;branch=gh-pages&amp;subPath=notebooks/14/4/Central_Limit_Theorem.ipynb">Interact</a>

    <div class="page__inner-wrap">
      
        <header>
          <!-- <h1 id="page-title" class="page__title" itemprop="headline">The Central Limit Theorem ###
</h1> -->
          
        </header>
      

      <section class="page__content" itemprop="text">
        
        <h3 id="the-central-limit-theorem">The Central Limit Theorem</h3>
<p>Very few of the data histograms that we have seen in this course have been bell shaped. When we have come across a bell shaped distribution, it has almost invariably been an empirical histogram of a statistic based on a random sample.</p>

<p>The examples below show two very different situations in which an approximate bell shape appears in such histograms.</p>

<h3 id="net-gain-in-roulette">Net Gain in Roulette</h3>
<p>In an earlier section, the bell appeared as the rough shape of the total amount of money we would make if we placed the same bet repeatedly on different spins of a roulette wheel.</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">wheel</span>
</code></pre></div></div>

<div>
<table border="1" class="dataframe">
    <thead>
        <tr>
            <th>Pocket</th> <th>Color</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>0     </td> <td>green</td>
        </tr>
        <tr>
            <td>00    </td> <td>green</td>
        </tr>
        <tr>
            <td>1     </td> <td>red  </td>
        </tr>
        <tr>
            <td>2     </td> <td>black</td>
        </tr>
        <tr>
            <td>3     </td> <td>red  </td>
        </tr>
        <tr>
            <td>4     </td> <td>black</td>
        </tr>
        <tr>
            <td>5     </td> <td>red  </td>
        </tr>
        <tr>
            <td>6     </td> <td>black</td>
        </tr>
        <tr>
            <td>7     </td> <td>red  </td>
        </tr>
        <tr>
            <td>8     </td> <td>black</td>
        </tr>
    </tbody>
</table>
<p>... (28 rows omitted)</p>
</div>

<p>Recall that the bet on red pays even money, 1 to 1. We defined the function <code class="highlighter-rouge">red_winnings</code> that returns the net winnings on one $1 bet on red. Specifically, the function takes a color as its argument and returns 1 if the color is red. For all other colors it returns -1.</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">red_winnings</span><span class="p">(</span><span class="n">color</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">color</span> <span class="o">==</span> <span class="s">'red'</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">1</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="o">-</span><span class="mi">1</span>
</code></pre></div></div>

<p>The table <code class="highlighter-rouge">red</code> shows each pocket’s winnings on red.</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">red</span> <span class="o">=</span> <span class="n">wheel</span><span class="o">.</span><span class="n">with_column</span><span class="p">(</span>
    <span class="s">'Winnings: Red'</span><span class="p">,</span> <span class="n">wheel</span><span class="o">.</span><span class="nb">apply</span><span class="p">(</span><span class="n">red_winnings</span><span class="p">,</span> <span class="s">'Color'</span><span class="p">)</span>
    <span class="p">)</span>
<span class="n">red</span>
</code></pre></div></div>

<div>
<table border="1" class="dataframe">
    <thead>
        <tr>
            <th>Pocket</th> <th>Color</th> <th>Winnings: Red</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>0     </td> <td>green</td> <td>-1           </td>
        </tr>
        <tr>
            <td>00    </td> <td>green</td> <td>-1           </td>
        </tr>
        <tr>
            <td>1     </td> <td>red  </td> <td>1            </td>
        </tr>
        <tr>
            <td>2     </td> <td>black</td> <td>-1           </td>
        </tr>
        <tr>
            <td>3     </td> <td>red  </td> <td>1            </td>
        </tr>
        <tr>
            <td>4     </td> <td>black</td> <td>-1           </td>
        </tr>
        <tr>
            <td>5     </td> <td>red  </td> <td>1            </td>
        </tr>
        <tr>
            <td>6     </td> <td>black</td> <td>-1           </td>
        </tr>
        <tr>
            <td>7     </td> <td>red  </td> <td>1            </td>
        </tr>
        <tr>
            <td>8     </td> <td>black</td> <td>-1           </td>
        </tr>
    </tbody>
</table>
<p>... (28 rows omitted)</p>
</div>

<p>Your net gain on one bet is one random draw from the <code class="highlighter-rouge">Winnings: Red</code> column. There is an 18/38 chance making $1, and a 20/38 chance of making -$1. This probability distribution is shown in the histogram below.</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">red</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s">'Winnings: Red'</span><span class="p">)</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">bins</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">1.6</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
</code></pre></div></div>

<p><img src="/jupyterhub-for-education-template/images/textbook/14/4/Central_Limit_Theorem_8_0.png" alt="png" /></p>

<p>Now suppose you bet many times on red. Your net winnings will be the sum of many draws made at random with replacement from the distribution above.</p>

<p>It will take a bit of math to list all the possible values of your net winnings along with all of their chances. We won’t do that; instead, we will approximate the probability distribution by simulation, as we have done all along in this course.</p>

<p>The code below simulates your net gain if you bet $1 on red on 400 different spins of the roulette wheel.</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">num_bets</span> <span class="o">=</span> <span class="mi">400</span>
<span class="n">repetitions</span> <span class="o">=</span> <span class="mi">10000</span>

<span class="n">net_gain_red</span> <span class="o">=</span> <span class="n">make_array</span><span class="p">()</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">repetitions</span><span class="p">):</span>
    <span class="n">spins</span> <span class="o">=</span> <span class="n">red</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">num_bets</span><span class="p">)</span>
    <span class="n">new_net_gain_red</span> <span class="o">=</span> <span class="n">spins</span><span class="o">.</span><span class="n">column</span><span class="p">(</span><span class="s">'Winnings: Red'</span><span class="p">)</span><span class="o">.</span><span class="nb">sum</span><span class="p">()</span>
    <span class="n">net_gain_red</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">net_gain_red</span><span class="p">,</span> <span class="n">new_net_gain_red</span><span class="p">)</span>


<span class="n">results</span> <span class="o">=</span> <span class="n">Table</span><span class="p">()</span><span class="o">.</span><span class="n">with_column</span><span class="p">(</span>
    <span class="s">'Net Gain on Red'</span><span class="p">,</span> <span class="n">net_gain_red</span>
    <span class="p">)</span>
</code></pre></div></div>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">results</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">bins</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">80</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
</code></pre></div></div>

<p><img src="/jupyterhub-for-education-template/images/textbook/14/4/Central_Limit_Theorem_11_0.png" alt="png" /></p>

<p>That’s a roughly bell shaped histogram, even though the distribution we are drawing from is nowhere near bell shaped.</p>

<p><strong>Center.</strong> The distribution is centered near -$20, roughly. To see why, note that your winnings will be $1 on about 18/38 of the bets, and -$1 on the remaining 20/38. So your average winnings per dollar bet will be roughly -5.26 cents:</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">average_per_bet</span> <span class="o">=</span> <span class="mi">1</span><span class="o">*</span><span class="p">(</span><span class="mi">18</span><span class="o">/</span><span class="mi">38</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="mi">20</span><span class="o">/</span><span class="mi">38</span><span class="p">)</span>
<span class="n">average_per_bet</span>
</code></pre></div></div>

<div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>-0.05263157894736842
</code></pre></div></div>

<p>So in 400 bets you expect that your net gain will be about -$21:</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="mi">400</span> <span class="o">*</span> <span class="n">average_per_bet</span>
</code></pre></div></div>

<div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>-21.052631578947366
</code></pre></div></div>

<p>For confirmation, we can compute the mean of the 10,000 simulated net gains:</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">results</span><span class="o">.</span><span class="n">column</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
</code></pre></div></div>

<div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>-20.9424
</code></pre></div></div>

<p><strong>Spread.</strong> Run your eye along the curve starting at the center and notice that the point of inflection is near 0. On a bell shaped curve, the SD is the distance from the center to a point of inflection. The center is roughly -$20, which means that the SD of the distribution is around $20.</p>

<p>In the next section we will see where the $20 comes from. For now, let’s confirm our observation by simply calculating the SD of the 10,000 simulated net gains:</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">results</span><span class="o">.</span><span class="n">column</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
</code></pre></div></div>

<div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>20.066486544485063
</code></pre></div></div>

<p><strong>Summary.</strong> The net gain in 400 bets is the sum of the 400 amounts won on each individual bet. The probability distribution of that sum is approximately normal, with an average and an SD that we can approximate.</p>

<h3 id="average-flight-delay">Average Flight Delay</h3>
<p>The table <code class="highlighter-rouge">united</code> contains data on departure delays of 13,825 United Airlines domestic flights out of San Francisco airport in the summer of 2015. As we have seen before, the distribution of delays has a long right-hand tail.</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">united</span> <span class="o">=</span> <span class="n">Table</span><span class="o">.</span><span class="n">read_table</span><span class="p">(</span><span class="n">path_data</span> <span class="o">+</span> <span class="s">'united_summer2015.csv'</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">united</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s">'Delay'</span><span class="p">)</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">bins</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">20</span><span class="p">,</span> <span class="mi">300</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
</code></pre></div></div>

<p><img src="/jupyterhub-for-education-template/images/textbook/14/4/Central_Limit_Theorem_23_0.png" alt="png" /></p>

<p>The mean delay was about 16.6 minutes and the SD was about 39.5 minutes. Notice how large the SD is, compared to the mean. Those large deviations on the right have an effect, even though they are a very small proportion of the data.</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mean_delay</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">united</span><span class="o">.</span><span class="n">column</span><span class="p">(</span><span class="s">'Delay'</span><span class="p">))</span>
<span class="n">sd_delay</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">united</span><span class="o">.</span><span class="n">column</span><span class="p">(</span><span class="s">'Delay'</span><span class="p">))</span>

<span class="n">mean_delay</span><span class="p">,</span> <span class="n">sd_delay</span>
</code></pre></div></div>

<div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(16.658155515370705, 39.480199851609314)
</code></pre></div></div>

<p>Now suppose we sampled 400 delays at random with replacement. You could sample without replacement if you like, but the results would be very similar to with-replacement sampling. If you sample a few hundred out of 13,825 without replacement, you hardly change the population each time you pull out a value.</p>

<p>In the sample, what could the average delay be? We expect it to be around 16 or 17, because that’s the population average; but it is likely to be somewhat off. Let’s see what we get by sampling. We’ll work with the table <code class="highlighter-rouge">delay</code> that only contains the column of delays.</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">delay</span> <span class="o">=</span> <span class="n">united</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s">'Delay'</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">delay</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">400</span><span class="p">)</span><span class="o">.</span><span class="n">column</span><span class="p">(</span><span class="s">'Delay'</span><span class="p">))</span>
</code></pre></div></div>

<div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>15.7625
</code></pre></div></div>

<p>The sample average varies according to how the sample comes out, so we will simulate the sampling process repeatedly and draw the empirical histogram of the sample average. That will be an approximation to the probability histogram of the sample average.</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sample_size</span> <span class="o">=</span> <span class="mi">400</span>
<span class="n">repetitions</span> <span class="o">=</span> <span class="mi">10000</span>

<span class="n">means</span> <span class="o">=</span> <span class="n">make_array</span><span class="p">()</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">repetitions</span><span class="p">):</span>
    <span class="n">sample</span> <span class="o">=</span> <span class="n">delay</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">sample_size</span><span class="p">)</span>
    <span class="n">new_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">sample</span><span class="o">.</span><span class="n">column</span><span class="p">(</span><span class="s">'Delay'</span><span class="p">))</span>
    <span class="n">means</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">means</span><span class="p">,</span> <span class="n">new_mean</span><span class="p">)</span>

<span class="n">results</span> <span class="o">=</span> <span class="n">Table</span><span class="p">()</span><span class="o">.</span><span class="n">with_column</span><span class="p">(</span>
    <span class="s">'Sample Mean'</span><span class="p">,</span> <span class="n">means</span>
<span class="p">)</span>
</code></pre></div></div>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">results</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">bins</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">25</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">))</span>
</code></pre></div></div>

<p><img src="/jupyterhub-for-education-template/images/textbook/14/4/Central_Limit_Theorem_31_0.png" alt="png" /></p>

<p>Once again, we see a rough bell shape, even though we are drawing from a very skewed distribution. The bell is centered somewhere between 16 ad 17, as we expect.</p>

<h3 id="central-limit-theorem">Central Limit Theorem</h3>

<p>The reason why the bell shape appears in such settings is a remarkable result of probability theory called the <strong>Central Limit Theorem</strong>.</p>

<p><strong>The Central Limit Theorem says that the probability distribution of the sum or average of a large random sample drawn with replacement will be roughly normal, <em>regardless of the distribution of the population from which the sample is drawn</em>.</strong></p>

<p>As we noted when we were studying Chebychev’s bounds, results that can be applied to random samples <em>regardless of the distribution of the population</em> are very powerful, because in data science we rarely know the distribution of the population.</p>

<p>The Central Limit Theorem makes it possible to make inferences with very little knowledge about the population, provided we have a large random sample. That is why it is central to the field of statistical inference.</p>

<h3 id="proportion-of-purple-flowers">Proportion of Purple Flowers</h3>
<p>Recall Mendel’s probability model for the colors of the flowers of a species of pea plant. The model says that the flower colors of the plants are like draws made at random with replacement from {Purple, Purple, Purple, White}.</p>

<p>In a large sample of plants, about what proportion will have purple flowers? We would expect the answer to be about 0.75, the proportion purple in the model. And, because proportions are means, the Central Limit Theorem says that the distribution of the sample proportion of purple plants is roughly normal.</p>

<p>We can confirm this by simulation. Let’s simulate the proportion of purple-flowered plants in a sample of 200 plants.</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">colors</span> <span class="o">=</span> <span class="n">make_array</span><span class="p">(</span><span class="s">'Purple'</span><span class="p">,</span> <span class="s">'Purple'</span><span class="p">,</span> <span class="s">'Purple'</span><span class="p">,</span> <span class="s">'White'</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">Table</span><span class="p">()</span><span class="o">.</span><span class="n">with_column</span><span class="p">(</span><span class="s">'Color'</span><span class="p">,</span> <span class="n">colors</span><span class="p">)</span>

<span class="n">model</span>
</code></pre></div></div>

<div>
<table border="1" class="dataframe">
    <thead>
        <tr>
            <th>Color</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>Purple</td>
        </tr>
        <tr>
            <td>Purple</td>
        </tr>
        <tr>
            <td>Purple</td>
        </tr>
        <tr>
            <td>White </td>
        </tr>
    </tbody>
</table>
</div>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">props</span> <span class="o">=</span> <span class="n">make_array</span><span class="p">()</span>

<span class="n">num_plants</span> <span class="o">=</span> <span class="mi">200</span>
<span class="n">repetitions</span> <span class="o">=</span> <span class="mi">10000</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">repetitions</span><span class="p">):</span>
    <span class="n">sample</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">num_plants</span><span class="p">)</span>
    <span class="n">new_prop</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">count_nonzero</span><span class="p">(</span><span class="n">sample</span><span class="o">.</span><span class="n">column</span><span class="p">(</span><span class="s">'Color'</span><span class="p">)</span> <span class="o">==</span> <span class="s">'Purple'</span><span class="p">)</span><span class="o">/</span><span class="n">num_plants</span>
    <span class="n">props</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">props</span><span class="p">,</span> <span class="n">new_prop</span><span class="p">)</span>
    
<span class="n">results</span> <span class="o">=</span> <span class="n">Table</span><span class="p">()</span><span class="o">.</span><span class="n">with_column</span><span class="p">(</span><span class="s">'Sample Proportion: 200'</span><span class="p">,</span> <span class="n">props</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">results</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">bins</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">0.65</span><span class="p">,</span> <span class="mf">0.85</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">))</span>
</code></pre></div></div>

<p><img src="/jupyterhub-for-education-template/images/textbook/14/4/Central_Limit_Theorem_37_0.png" alt="png" /></p>

<p>There’s that normal curve again, as predicted by the Central Limit Theorem, centered at around 0.75 just as you would expect.</p>

<p>How would this distribution change if we increased the sample size? Let’s run the code again with a sample size of 800, and collect the results of simulations in the same table in which we collected simulations based on a sample size of 200. We will keep the number of <code class="highlighter-rouge">repetitions</code> the same as before so that the two columns have the same length.</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">props2</span> <span class="o">=</span> <span class="n">make_array</span><span class="p">()</span>

<span class="n">num_plants</span> <span class="o">=</span> <span class="mi">800</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">repetitions</span><span class="p">):</span>
    <span class="n">sample</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">num_plants</span><span class="p">)</span>
    <span class="n">new_prop</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">count_nonzero</span><span class="p">(</span><span class="n">sample</span><span class="o">.</span><span class="n">column</span><span class="p">(</span><span class="s">'Color'</span><span class="p">)</span> <span class="o">==</span> <span class="s">'Purple'</span><span class="p">)</span><span class="o">/</span><span class="n">num_plants</span>
    <span class="n">props2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">props2</span><span class="p">,</span> <span class="n">new_prop</span><span class="p">)</span>
    
<span class="n">results</span> <span class="o">=</span> <span class="n">results</span><span class="o">.</span><span class="n">with_column</span><span class="p">(</span><span class="s">'Sample Proportion: 800'</span><span class="p">,</span> <span class="n">props2</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">results</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">bins</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">0.65</span><span class="p">,</span> <span class="mf">0.85</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">))</span>
</code></pre></div></div>

<p><img src="/jupyterhub-for-education-template/images/textbook/14/4/Central_Limit_Theorem_40_0.png" alt="png" /></p>

<p>Both distributions are approximately normal but one is narrower than the other. The proportions based on a sample size of 800 are more tightly clustered around 0.75 than those from a sample size of 200. Increasing the sample size has decreased the variability in the sample proportion.</p>

<p>This should not be surprising.  We have leaned many times on the intuition that a larger sample size generally reduces the variability of a statistic.  However, in the case of a sample average, we can <em>quantify</em> the relationship between sample size and variability.</p>

<p>Exactly how does the sample size affect the variability of a sample average or proportion? That is the question we will examine in the next section.</p>

        
      </section>

      <footer class="page__meta">
        
        


        
      </footer>

      

      
  <nav class="pagination">
    
      <a href="/jupyterhub-for-education-template/textbook/14/3/SD_and_the_Normal_Curve" class="pagination--pager" title="14.3 The SD and the Normal Curve
">Previous</a>
    
    
      <a href="/jupyterhub-for-education-template/textbook/14/5/Variability_of_the_Sample_Mean" class="pagination--pager" title="14.5 The Variability of the Sample Mean
">Next</a>
    
  </nav>

    </div>

    
  </article>

  
  
</div>

    </div>

    

    <div class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    
    
    
    
    
    
    <li><a href="/jupyterhub-for-education-template/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2018 Berkeley DSEP. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/jupyterhub-for-education-template/assets/js/main.min.js"></script>
  <script src="https://use.fontawesome.com/releases/v5.0.12/js/all.js"></script>








  </body>
</html>
